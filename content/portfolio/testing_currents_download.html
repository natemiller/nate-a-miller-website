---
title: "Function for Uploading HYCOM currents to BigQuery"
output:
  html_document:
    highlight: tango
    keep_md: yes
    theme: lumen
---



<pre class="r"><code>library(tidyverse)
library(lubridate)
library(ncdf4)
library(purrr)</code></pre>
<p>This is an initial function to pull currents data from the <a href="https://www.hycom.org/data/glbv0pt08/expt-93pt0">HYCOM GOFS 3.1: 41-layer HYCOM + NCODA Global 1/12° Analysis</a>. Data availability varies
by time period but this particular function pulls for data that goes back to at least <code>2018-01-01</code>.
Slight modifications to the <code>curl</code> url may be necessary to pull earlier currents data.</p>
<p>The data resolution varied by latitude in the following manner. The grid is 0.08 deg lon x 0.08 deg lat
between 40S-40N. Poleward of 40S/40N, the grid is 0.08 deg lon x 0.04 deg lat. It spans 80S to 90N.
The temporal resolution is every 3 hours starting at 0:00 and going to 24:00, thus
there can be repeats at midnight. To eliminate this, this function starts each day at 0:00 and the last
time point is 21:00, leaving 24:00 to be represented by 0:00 of the following day.</p>
<p>The URL provides the daily data in a NetCDF format (<code>.nc</code>), which then needs to be processed.
The data is stored as two, 3251 x 4500 x 9 array (the depth being the time points),
one array for the northward currents and one array for the eastward currents.</p>
<p>I have combined the two current vectors into the resultant vector as
<code>sqrt(x_current^2 + y_current^2)</code></p>
<p>The function below is designed to loop through a series of dates to bulk upload all day for each day
using <code>bq load</code>.</p>
<div id="issues" class="section level3">
<h3>ISSUES:</h3>
<p>Currently the daily table size appears to be too large to upload in a timely manner
to Big Query. The total table is 117,036,000 rows per day and take ~45 - 60 minutes
to upload and typically times out.</p>
<p>I am exploring other options for upload (possibly from GCS) or for making the uploads
smaller.</p>
</div>
<div id="update" class="section level3">
<h3>UPDATE:</h3>
<p>I reformulated the code to use <code>data.table::rbindlist</code> for binding the 3-hourly dataframes together. Previously I was
using <code>rbind</code> which was not smart and got slower as each data frame was added. The data.table implementation
also appears to be ~2x faster than <code>dplyr::bind_rows</code>. I also added <code>data.table::fwrite</code> for
saving the .CSV prior to uploading to BQ. This implementation took a save process (for a 4.3GB file) that required
nearly an hour using <code>readr::read_csv</code> and reduced it to ~20 seconds. I also removed the for loops by using <code>purrr</code>,
which makes the code a bit tidier, but doesn’t really have an effect on the performance. The slowest portion of the entire
process remains <code>bq load</code> and I don’t have a means of making that process any faster.</p>
<pre class="r"><code># function to process individual time points within each day
process_timepoint &lt;- function(index, x_current.array, y_current.array,lonlat, nlat, nlon, date, hour_seq) {
    print(glue::glue(&#39;Processing Hour: {hour_seq[index]}&#39;))
    # select specified time point
    x_current.array_day &lt;- x_current.array[,, index]
    y_current.array_day &lt;- y_current.array[,, index]
    
    # convert to long vector
    x_current.vec.long &lt;- as.numeric(as.vector(x_current.array_day))
    y_current.vec.long &lt;- as.numeric(as.vector(y_current.array_day))
    
    # convert variable to matrix the length of lat/lon
    x_current.mat &lt;- matrix(x_current.vec.long, nrow = nlon * nlat, ncol = 1)
    y_current.mat &lt;- matrix(y_current.vec.long, nrow = nlon * nlat, ncol = 1)
    
    #expand lat/lon and bind to wind speed variable
    #lonlat &lt;- expand.grid(lon, lat)
    x_current_df &lt;- data.frame(cbind(lonlat, x_current.mat))
    y_current_df &lt;- data.frame(cbind(lonlat, y_current.mat))
    
    #rename fields
    names(x_current_df) &lt;- c(&#39;lon&#39;,&#39;lat&#39;,&#39;x_current&#39;)
    names(y_current_df) &lt;- c(&#39;lon&#39;,&#39;lat&#39;,&#39;y_current&#39;)
    
    x_current_df$lon &lt;- c(x_current_df$lon)
    x_current_df$lat &lt;- c(x_current_df$lat)
    
    y_current_df$lon &lt;- c(y_current_df$lon)
    y_current_df$lat &lt;- c(y_current_df$lat)
    
    #combine x_current and y_current into combined current vector
     total_current_vector &lt;-  x_current_df %&gt;%
        dplyr::mutate(date = date,
                      lat = round(lat, 2),
                      lon = round(lon, 2),
                      current = sqrt(x_current * x_current +
                                     y_current_df$y_current * y_current_df$y_current),
                      current = ifelse(is.na(current), &#39;&#39;, current),
                      hour = hour_seq[index]) %&gt;%
       dplyr::select(-x_current)
  
}</code></pre>
<pre class="r"><code># function to pull data from HYCOM by day, processes all time points within a
# day and append data to Big Query table.
process_currents &lt;- function(date) {
  
  temp_list &lt;- list()
  
  next_date &lt;- as.Date(date, tz = &#39;UTC&#39;) + lubridate::days(1)
  
  print(glue::glue(&#39;Downloading Date: {date}&#39; ))
  
  #curl command to download data to working directory
  curl::curl_download(glue::glue(
&#39;http://ncss.hycom.org/thredds/ncss/GLBv0.08/expt_93.0/uv3z?var=water_u&amp;var=water_v&amp;north=90.0000&amp;west=0.0000&amp;east=359.9200&amp;south=-80.0000&amp;disableLLSubset=on&amp;disableProjSubset=on&amp;horizStride=1&amp;time_start={date}T00%3A00%3A00Z&amp;time_end={next_date}T00%3A00%3A00Z&amp;timeStride=1&amp;vertCoord=0.0&amp;addLatLon=true&amp;accept=netcdf&#39;),&#39;testing_netcdf.nc&#39;)

  currents_file &lt;- ncdf4::nc_open(&#39;./testing_netcdf.nc&#39;)
  
  lon &lt;- ncdf4::ncvar_get(currents_file, &quot;lon&quot;)
  nlon &lt;- dim(lon)

  lat &lt;- ncdf4::ncvar_get(currents_file, &quot;lat&quot;)
  nlat &lt;- dim(lat)
  
  #get times
  time &lt;- ncdf4::ncvar_get(currents_file, &quot;time&quot;)
  
  tunits &lt;- ncdf4::ncatt_get(currents_file,&quot;time&quot;,&quot;units&quot;)
  
  # convert time -- split the time units string into fields
  tustr &lt;- strsplit(tunits$value, &quot; &quot;)
  tdstr &lt;- strsplit(unlist(tustr)[3], &quot;-&quot;)
  tmonth &lt;- as.integer(unlist(tdstr)[2])
  tday &lt;- as.integer(unlist(tdstr)[3])
  tyear &lt;- as.integer(unlist(tdstr)[1])
  new_time &lt;- as.POSIXct(paste0(tyear,&#39;-&#39;,tmonth,&#39;-&#39;,tday), tz = &#39;UTC&#39;) + 
                              lubridate::hours(time)
  
  hour_seq &lt;- data.frame(time_c = new_time) %&gt;%
    mutate(date_c = as.Date(time_c, tz = &#39;UTC&#39;)) %&gt;%
    filter(date_c == as.Date(date, tz = &#39;UTC&#39;)) %&gt;%
    mutate(time_hour = lubridate::hour(time_c)) %&gt;%
    pull(time_hour)
  
  #wind speed variables (u, v)
  x_current.array &lt;- ncdf4::ncvar_get(currents_file, &#39;water_u&#39;)
  y_current.array &lt;- ncdf4::ncvar_get(currents_file, &#39;water_v&#39;)
  
  # NA value
  x.current.fillvalue &lt;- ncdf4::ncatt_get(currents_file, &#39;water_u&#39;, &quot;_FillValue&quot;)
  y.current.fillvalue &lt;- ncdf4::ncatt_get(currents_file, &#39;water_v&#39;, &quot;_FillValue&quot;)
  
  #replace fill value with NA
  x_current.array[x_current.array == x.current.fillvalue$value] &lt;- NA_real_
  y_current.array[y_current.array == y.current.fillvalue$value] &lt;- NA_real_
  
  #specify lon/lat grid
  lat_grid &lt;- c(seq(-80,-40.04,length.out = 1000), 
                seq(-40,40,length.out = 1001), 
                seq(40.04,90,length.out = 1250))
  lon_grid &lt;- seq(0,359.92, length.out = 4500)
  lonlat &lt;- expand.grid(lon_grid, lat_grid)
  
  #for each time point (ignoring the last timepoint of the day (24:00))
 
  temp_list &lt;- setNames(purrr::map(.x = seq_along(hour_seq), 
             .f = process_timepoint,
             x_current.array, 
             y_current.array,
             lonlat,
             nlat,
             nlon,
             date,
             hour_seq), nm = hour_seq)
  
  
  #running_total
  #save table
  print(&#39;Saving File&#39;)
  day_total_df &lt;- data.table::rbindlist(temp_list)
  data.table::fwrite(day_total_df, &#39;./testing_currents.csv&#39;)
  print(&#39;Uploading to BigQuery&#39;)
   #upload to Big Query (in RStudio need complete path to bq function)
  command = &#39;bq load --skip_leading_rows=1 --noreplace world-fishing-827:scratch_nate.testing_currents testing_currents.csv lon:float,lat:float,date:date,current:float,hour:integer&#39;
  system(command)
   # delete .CSV 
  system(&#39;rm testing_currents.csv&#39;)
      
}</code></pre>
<div id="example-of-function-for-1-day" class="section level4">
<h4>Example of function for 1 day</h4>
<pre class="r"><code>process_currents(&#39;2019-01-01&#39;)</code></pre>
</div>
<div id="example-of-function-for-3-days" class="section level4">
<h4>Example of function for 3 days</h4>
<pre class="r"><code>date_sequence &lt;- seq.Date(as.Date(&#39;2018-01-01&#39;, tz = &#39;UTC&#39;),
                          as.Date(&#39;2018-01-03&#39;, tz = &#39;UTC&#39;), 
                          by = &#39;day&#39;)

purrr::map(.x = date_sequence,
           .f = process_currents)</code></pre>
</div>
</div>
